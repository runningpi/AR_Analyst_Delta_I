# AR Analyst Delta I Project

**Quantifying Novel Insights in Financial Analyst Reports**

This project analyzes analyst reports to calculate the **information delta (Î”I)** - the truly novel information that analysts provide beyond what's already disclosed in official company documents.

## ğŸ¯ Project Formula

```
Î”I = Î”AR - Î”Î£

Where:
  Î”AR = Information in Analyst Reports
  Î”Î£  = Information in Company Documents (10-K, 10-Q, earnings calls, etc.)
  Î”I  = Information Delta (novel analyst insights)
```

## ğŸ“ Project Structure

```
AR_Analyst_Delta_I/
â”‚
â”œâ”€â”€ ğŸ“Š Analysis_Pipeline/          # â­ Main Production Pipeline
â”‚   â”‚
â”‚   â”‚   Complete automated pipeline for analyzing analyst reports
â”‚   â”‚   against company documents using OCR, LLM classification,
â”‚   â”‚   RAG matching, and evaluation.
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ¯ core/                   # Orchestration & data models
â”‚   â”œâ”€â”€ ğŸ“‚ Decomposition_AR/       # OCR & sentence classification
â”‚   â”œâ”€â”€ ğŸ“‚ RAG_and_knowledgebase/  # DS-RAG knowledge base & matching
â”‚   â”œâ”€â”€ ğŸ“‚ Evaluation/             # LLM-based evaluation
â”‚   â”œâ”€â”€ ğŸ“‚ Analysis/               # Reports & statistics
â”‚   â””â”€â”€ ğŸ“„ README.md               # Full pipeline documentation
â”‚
â”œâ”€â”€ ğŸ§ª Playground/                 # Experimental & Development
â”‚   â”œâ”€â”€ docling_ocr/               # Docling OCR experiments
â”‚   â”œâ”€â”€ mistral_OCR/               # Mistral OCR experiments
â”‚   â””â”€â”€ GPT_information_extraction/ # LLM extraction prototypes
â”‚
â””â”€â”€ ğŸ“ data/                       # Input datasets
    â””â”€â”€ [COMPANY]/[QUARTER]/
        â”œâ”€â”€ analyst_report/        # Analyst reports (PDFs)
        â””â”€â”€ company_reports/       # Company documents (PDFs)
```

## ğŸš€ Getting Started

### Quick Start

The **Analysis Pipeline** is the main production system. To get started:

```bash
# Navigate to the pipeline
cd Analysis_Pipeline/

# Follow the setup instructions
cat README.md
```

### What Does It Do?

The Analysis Pipeline:

1. **ğŸ“„ Extracts** text from analyst report PDFs using Docling OCR
2. **ğŸ·ï¸ Classifies** sentences into categories (corporate info, market info, analyst interpretation)
3. **ğŸ” Matches** each sentence against a knowledge base of company documents using DS-RAG
4. **âš–ï¸ Evaluates** whether sentences are Supported, Contradicted, or Novel using GPT-4
5. **ğŸ“Š Generates** comprehensive reports with coverage statistics and insights

### Key Output

```
Coverage Analysis Example:
â”œâ”€â”€ Supported:     42 (27.8%) â† Backed by company documents
â”œâ”€â”€ Not Supported: 107 (70.9%) â† Novel analyst insights (Î”I) â­
â””â”€â”€ Contradicted:  2 (1.3%)   â† Conflicts with official data
```

**High "Not Supported" percentage = High information delta = Novel insights!**

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| **[Analysis_Pipeline/README.md](Analysis_Pipeline/README.md)** | Complete pipeline documentation, installation, usage |
| **[Analysis_Pipeline/WORKFLOW_DIAGRAM.md](Analysis_Pipeline/WORKFLOW_DIAGRAM.md)** | Detailed technical workflow diagrams |
| **[Analysis_Pipeline/settings.config](Analysis_Pipeline/settings.config)** | Configuration file for data paths |

## ğŸ› ï¸ Technical Stack

- **OCR**: Docling (PDF text extraction)
- **RAG**: DS-RAG (knowledge base & retrieval)
- **LLM**: GPT-4o-mini (classification & evaluation)
- **Embeddings**: OpenAI text-embedding-3-small
- **Reranking**: Cohere Rerank (optional)
- **Language**: Python 3.10+

## ğŸ“Š Use Cases

### Research Applications

1. **Analyst Alpha Measurement**: Quantify how much novel information analysts provide
2. **Information Quality**: Assess accuracy of analyst claims vs. company disclosures
3. **Coverage Gaps**: Identify areas where company disclosures are insufficient
4. **Contradiction Detection**: Find inconsistencies between analyst reports and official data

### Example Research Questions

- Do sell-side analysts provide novel information beyond company disclosures?
- What percentage of analyst claims are independently verifiable?
- Which analysts consistently provide the highest information delta?
- Are certain sections (e.g., risk factors) more novel than others?

## ğŸ“ Academic Context

This project supports research in:
- **Financial Information Economics**: Measuring information production by intermediaries
- **Market Efficiency**: Testing whether analysts add information to markets
- **Corporate Disclosure**: Identifying gaps in company communications
- **NLP for Finance**: Applying modern AI to financial text analysis

## ğŸ—ï¸ Development Workflow

### Production Pipeline

```bash
cd Analysis_Pipeline/
python analyse_delta_i_for_one_AR.py
```

### Experimentation

```bash
cd Playground/
# Explore different OCR methods, classification prompts, etc.
```

### Adding New Data

```bash
# Add company data
mkdir -p data/COMPANY_NAME/QUARTER/company_reports/
# Copy PDFs to company_reports/

# Add analyst report
mkdir -p data/COMPANY_NAME/QUARTER/analyst_report/
# Copy PDF to analyst_report/

# Update Analysis_Pipeline/settings.config
# Then run pipeline
```

## ğŸ”§ Configuration

Edit `Analysis_Pipeline/settings.config`:

```ini
env_file=.env
analyst_report=data/COMPANY/QUARTER/analyst_report/report.pdf
company_data_dir=data/COMPANY/QUARTER/company_reports
```

## ğŸ“ˆ Performance

- **First Run**: ~3-5 minutes (includes OCR, LLM calls, KB indexing)
- **Cached Runs**: ~0.2 seconds (loads cached results)
- **Cost**: ~$0.50-1.00 per analyst report (OpenAI API)

**Intelligent Caching**: All stages (OCR, classification, matching, evaluation) are cached to prevent redundant processing.

## ğŸ¤ Contributing

### Project Maintainers
- **ManÃº Weissel** - Goethe University Research

### Development Guidelines
- Production code goes in `Analysis_Pipeline/`
- Experiments go in `Playground/`
- All outputs are cached in stage-specific subdirectories
- Follow the logging standards from DataNXT

## ğŸ“„ License

Internal use only - Goethe University Research

---

## ğŸ¯ Next Steps

1. **Read the full documentation**: [Analysis_Pipeline/README.md](Analysis_Pipeline/README.md)
2. **Set up the environment**: Follow Quick Start guide
3. **Run your first analysis**: Process an analyst report
4. **Review the results**: Check the Analysis/output/ directory

## ğŸ“§ Contact

For questions about this project, contact the research team at Goethe University.

---

**Built with â¤ï¸ for financial research at Goethe University**

